{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8431a26f",
   "metadata": {},
   "source": [
    "# Web Scraping data from books website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6476ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "# Import required libraries\n",
    "from lxml import etree\n",
    "import json\n",
    "from typing import Dict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f60f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure request was successful\n",
    "    return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract category from book detail page\n",
    "def get_book_category(book_url):\n",
    "    soup = get_soup(book_url)\n",
    "    # Breadcrumb: Home > Books > Category > Book Title\n",
    "    breadcrumb = soup.find('ul', class_='breadcrumb')\n",
    "    if breadcrumb:\n",
    "        links = breadcrumb.find_all('a')\n",
    "        # 0: Home, 1: Books, 2: Category\n",
    "        if len(links) >= 3:\n",
    "            return links[2].text.strip()\n",
    "    return 'Unknown'\n",
    "\n",
    "def scrape_books():\n",
    "    base_url = \"https://books.toscrape.com/\"\n",
    "    books_data = []\n",
    "    page = 1\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Construct URL for each page\n",
    "            if page == 1:\n",
    "                url = base_url\n",
    "            else:\n",
    "                url = base_url + f'catalogue/page-{page}.html'\n",
    "            \n",
    "            print(f\"Scraping page {page}...\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            # Break if page not found\n",
    "            if response.status_code == 404:\n",
    "                break\n",
    "                \n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all books on the page\n",
    "            books = soup.select('article.product_pod')\n",
    "            \n",
    "            if not books:\n",
    "                break\n",
    "                \n",
    "            for book in books:\n",
    "                product_url = base_url + \"catalogue/\" + book.select_one('h3 a')['href'].replace('../', '').replace(\"catalogue\", \"\")\n",
    "                book_data = {\n",
    "                    \"title\": book.h3.a['title'],\n",
    "                    \"price\": book.select_one('p.price_color').text.strip(),\n",
    "                    \"rating\": book.select_one('p.star-rating')['class'][1],\n",
    "                    \"availability\": book.select_one('p.availability').text.strip(),\n",
    "                    \"image_url\": base_url + book.select_one('img')['src'].replace('../', ''),\n",
    "                    \"product_url\": product_url,\n",
    "                    \"category\": get_book_category(product_url)\n",
    "                }\n",
    "                books_data.append(book_data)\n",
    "            \n",
    "            # Add delay to be respectful to the server\n",
    "            # time.sleep(1)\n",
    "            page += 1\n",
    "        \n",
    "        # Save to JSON file\n",
    "        with open('books_datav2.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(books_data, f, indent=4, ensure_ascii=False)\n",
    "            \n",
    "        print(\"\\nData successfully scraped and saved to books_data.json\")\n",
    "        return books_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    books_data = scrape_books()\n",
    "    if books_data:\n",
    "        print(f\"\\nTotal books scraped: {len(books_data)}\")\n",
    "        print(\"\\nSample Book Data:\")\n",
    "        print(json.dumps(books_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e876bd26",
   "metadata": {},
   "source": [
    "## Converting the JSON file to XML and validation using RelaxNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d92d2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted JSON to XML. Saved as books_datav2.xml\n",
      "✅ XML document is valid against the schema\n",
      "XML structure is valid\n",
      "\n",
      "Conversion and validation successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_book_element(doc: etree.ElementTree, book_data: Dict, inde) -> etree.Element:\n",
    "    \"\"\"Create an XML element for a book from its JSON data\"\"\"\n",
    "    book = etree.SubElement(doc, \"book\")\n",
    "    \n",
    "    # Add all book details as sub-elements\n",
    "    for key, value in book_data.items():\n",
    "        elem = etree.SubElement(book, key.replace(\"_\", \"-\"))\n",
    "        elem.text = str(value)\n",
    "    \n",
    "    return book\n",
    "\n",
    "def validate_xml(xml_file: str, schema_file: str) -> bool:\n",
    "    \"\"\"\n",
    "    Validate XML against RelaxNG schema\n",
    "    Returns True if valid, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verify files exist\n",
    "        if not os.path.exists(xml_file):\n",
    "            raise FileNotFoundError(f\"XML file not found: {xml_file}\")\n",
    "        if not os.path.exists(schema_file):\n",
    "            raise FileNotFoundError(f\"Schema file not found: {schema_file}\")\n",
    "            \n",
    "        # Parse the XML file and schema\n",
    "        xml_doc = etree.parse(xml_file)\n",
    "        relaxng_doc = etree.RelaxNG(etree.parse(schema_file))\n",
    "            \n",
    "        # Validate and return result\n",
    "        is_valid = relaxng_doc.validate(xml_doc)\n",
    "        \n",
    "        if is_valid:\n",
    "            print(\"✅ XML document is valid against the schema\")\n",
    "        else:\n",
    "            print(\"❌ XML validation failed!\")\n",
    "            for error in relaxng_doc.error_log:\n",
    "                print(f\"Line {error.line}: {error.message}\")\n",
    "            \n",
    "        return is_valid\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Validation error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def json_to_xml(json_file: str, xml_file: str):\n",
    "    \"\"\"Convert JSON book data to XML format and validate\"\"\"\n",
    "    try:\n",
    "        # Read JSON data\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            books_data = json.load(f)\n",
    "        \n",
    "        # Create root element\n",
    "        root = etree.Element(\"books\")\n",
    "        \n",
    "        # Add each book to the XML tree\n",
    "        for i, book_data in enumerate(books_data):\n",
    "            create_book_element(root, book_data, i)\n",
    "        \n",
    "        # Create XML tree and save to file\n",
    "        tree = etree.ElementTree(root)\n",
    "        tree.write(\n",
    "            xml_file, \n",
    "            pretty_print=True, \n",
    "            encoding='utf-8', \n",
    "            xml_declaration=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully converted JSON to XML. Saved as {xml_file}\")\n",
    "        \n",
    "        # Validate the XML\n",
    "        if validate_xml(xml_file, 'books_schema.rng'):\n",
    "            print(\"XML structure is valid\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"XML structure is invalid\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run the conversion and validation\n",
    "if __name__ == \"__main__\":\n",
    "    result = json_to_xml('books_datav2.json', 'books_datav2.xml')\n",
    "    print(f\"\\nConversion and validation {'successful' if result else 'failed'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5214ccfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typecode=37\n",
      "item=A Light in the Attic\n",
      "typecode=37\n",
      "item=Tipping the Velvet\n",
      "typecode=37\n",
      "item=Soumission\n",
      "typecode=37\n",
      "item=Sharp Objects\n",
      "typecode=37\n",
      "item=Sapiens: A Brief History of Humankind\n",
      "typecode=37\n",
      "item=The Requiem Red\n",
      "typecode=37\n",
      "item=The Dirty Little Secrets of Getting Your Dream Job\n",
      "typecode=37\n",
      "item=The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull\n",
      "typecode=37\n",
      "item=The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics\n",
      "typecode=37\n",
      "item=The Black Maria\n"
     ]
    }
   ],
   "source": [
    "from BaseXClient import BaseXClient\n",
    "\n",
    "try:\n",
    "    # Connect to BaseX server\n",
    "    session = BaseXClient.Session('localhost', 1984, 'admin', 'admin')\n",
    "    session.execute(\"open BookCatalog\")\n",
    "\n",
    "    # Drop old db if exists\n",
    "    # session.execute(\"drop db BookCatalog\")\n",
    "\n",
    "    # # Load XML content\n",
    "    # with open('books_datav2.xml', 'r', encoding='utf-8') as f:\n",
    "    #     xml_content = f.read()\n",
    "\n",
    "    # # Create new database\n",
    "    # session.execute(\"create db BookCatalog \" + xml_content)\n",
    "\n",
    "    # Query database\n",
    "    query = session.query(\"\"\"\n",
    "        for $b in /books/item\n",
    "        where number(substring-after($b/price, '£')) < 315\n",
    "        return data($b/title/text())\n",
    "    \"\"\")\n",
    "\n",
    "    # Print results\n",
    "    for typecode, item in query.iter():\n",
    "        print(\"typecode=%d\" % typecode)\n",
    "        print(\"item=%s\" % str(item))\n",
    "\n",
    "    # Close query and session\n",
    "    query.close()\n",
    "    session.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    query.close()\n",
    "    session.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef89a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
