{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8431a26f",
   "metadata": {},
   "source": [
    "# Web Scraping data from books website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6476ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Scraping page 32...\n",
      "Scraping page 33...\n",
      "Scraping page 34...\n",
      "Scraping page 35...\n",
      "Scraping page 36...\n",
      "Scraping page 37...\n",
      "Scraping page 38...\n",
      "Scraping page 39...\n",
      "Scraping page 40...\n",
      "Scraping page 41...\n",
      "Scraping page 42...\n",
      "Scraping page 43...\n",
      "Scraping page 44...\n",
      "Scraping page 45...\n",
      "Scraping page 46...\n",
      "Scraping page 47...\n",
      "Scraping page 48...\n",
      "Scraping page 49...\n",
      "Scraping page 50...\n",
      "Scraping page 51...\n",
      "\n",
      "Data successfully scraped and saved to books_data.json\n",
      "\n",
      "Total books scraped: 1000\n",
      "\n",
      "Sample Book Data:\n",
      "{\n",
      "  \"title\": \"A Light in the Attic\",\n",
      "  \"price\": \"\\u00a351.77\",\n",
      "  \"rating\": \"Three\",\n",
      "  \"availability\": \"In stock\",\n",
      "  \"image_url\": \"https://books.toscrape.com/media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\",\n",
      "  \"product_url\": \"https://books.toscrape.com/catalogue//a-light-in-the-attic_1000/index.html\",\n",
      "  \"category\": \"Poetry\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure request was successful\n",
    "    return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract category from book detail page\n",
    "def get_book_category(book_url):\n",
    "    soup = get_soup(book_url)\n",
    "    # Breadcrumb: Home > Books > Category > Book Title\n",
    "    breadcrumb = soup.find('ul', class_='breadcrumb')\n",
    "    if breadcrumb:\n",
    "        links = breadcrumb.find_all('a')\n",
    "        # 0: Home, 1: Books, 2: Category\n",
    "        if len(links) >= 3:\n",
    "            return links[2].text.strip()\n",
    "    return 'Unknown'\n",
    "\n",
    "def scrape_books():\n",
    "    base_url = \"https://books.toscrape.com/\"\n",
    "    books_data = []\n",
    "    page = 1\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Construct URL for each page\n",
    "            if page == 1:\n",
    "                url = base_url\n",
    "            else:\n",
    "                url = base_url + f'catalogue/page-{page}.html'\n",
    "            \n",
    "            print(f\"Scraping page {page}...\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            # Break if page not found\n",
    "            if response.status_code == 404:\n",
    "                break\n",
    "                \n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all books on the page\n",
    "            books = soup.select('article.product_pod')\n",
    "            \n",
    "            if not books:\n",
    "                break\n",
    "                \n",
    "            for book in books:\n",
    "                product_url = base_url + \"catalogue/\" + book.select_one('h3 a')['href'].replace('../', '').replace(\"catalogue\", \"\")\n",
    "                book_data = {\n",
    "                    \"title\": book.h3.a['title'],\n",
    "                    \"price\": book.select_one('p.price_color').text.strip(),\n",
    "                    \"rating\": book.select_one('p.star-rating')['class'][1],\n",
    "                    \"availability\": book.select_one('p.availability').text.strip(),\n",
    "                    \"image_url\": base_url + book.select_one('img')['src'].replace('../', ''),\n",
    "                    \"product_url\": product_url,\n",
    "                    \"category\": get_book_category(product_url)\n",
    "                }\n",
    "                books_data.append(book_data)\n",
    "            \n",
    "            # Add delay to be respectful to the server\n",
    "            # time.sleep(1)\n",
    "            page += 1\n",
    "        \n",
    "        # Save to JSON file\n",
    "        with open('books_datav2.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(books_data, f, indent=4, ensure_ascii=False)\n",
    "            \n",
    "        print(\"\\nData successfully scraped and saved to books_data.json\")\n",
    "        return books_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    books_data = scrape_books()\n",
    "    if books_data:\n",
    "        print(f\"\\nTotal books scraped: {len(books_data)}\")\n",
    "        print(\"\\nSample Book Data:\")\n",
    "        print(json.dumps(books_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e876bd26",
   "metadata": {},
   "source": [
    "## Converting the JSON file to XML and validation using RelaxNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d92d2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted JSON to XML. Saved as books_datav2.xml\n",
      "✅ XML document is valid against the schema\n",
      "XML structure is valid\n",
      "\n",
      "Conversion and validation successful\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from lxml import etree\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import os\n",
    "\n",
    "def create_book_element(doc: etree.ElementTree, book_data: Dict, inde) -> etree.Element:\n",
    "    \"\"\"Create an XML element for a book from its JSON data\"\"\"\n",
    "    book = etree.SubElement(doc, \"book\")\n",
    "    \n",
    "    # Add all book details as sub-elements\n",
    "    for key, value in book_data.items():\n",
    "        elem = etree.SubElement(book, key.replace(\"_\", \"-\"))\n",
    "        elem.text = str(value)\n",
    "    \n",
    "    return book\n",
    "\n",
    "def validate_xml(xml_file: str, schema_file: str) -> bool:\n",
    "    \"\"\"\n",
    "    Validate XML against RelaxNG schema\n",
    "    Returns True if valid, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verify files exist\n",
    "        if not os.path.exists(xml_file):\n",
    "            raise FileNotFoundError(f\"XML file not found: {xml_file}\")\n",
    "        if not os.path.exists(schema_file):\n",
    "            raise FileNotFoundError(f\"Schema file not found: {schema_file}\")\n",
    "            \n",
    "        # Parse the XML file and schema\n",
    "        xml_doc = etree.parse(xml_file)\n",
    "        relaxng_doc = etree.RelaxNG(etree.parse(schema_file))\n",
    "            \n",
    "        # Validate and return result\n",
    "        is_valid = relaxng_doc.validate(xml_doc)\n",
    "        \n",
    "        if is_valid:\n",
    "            print(\"✅ XML document is valid against the schema\")\n",
    "        else:\n",
    "            print(\"❌ XML validation failed!\")\n",
    "            for error in relaxng_doc.error_log:\n",
    "                print(f\"Line {error.line}: {error.message}\")\n",
    "            \n",
    "        return is_valid\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Validation error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def json_to_xml(json_file: str, xml_file: str):\n",
    "    \"\"\"Convert JSON book data to XML format and validate\"\"\"\n",
    "    try:\n",
    "        # Read JSON data\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            books_data = json.load(f)\n",
    "        \n",
    "        # Create root element\n",
    "        root = etree.Element(\"books\")\n",
    "        \n",
    "        # Add each book to the XML tree\n",
    "        for i, book_data in enumerate(books_data):\n",
    "            create_book_element(root, book_data, i)\n",
    "        \n",
    "        # Create XML tree and save to file\n",
    "        tree = etree.ElementTree(root)\n",
    "        tree.write(\n",
    "            xml_file, \n",
    "            pretty_print=True, \n",
    "            encoding='utf-8', \n",
    "            xml_declaration=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully converted JSON to XML. Saved as {xml_file}\")\n",
    "        \n",
    "        # Validate the XML\n",
    "        if validate_xml(xml_file, 'books_schema.rng'):\n",
    "            print(\"XML structure is valid\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"XML structure is invalid\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run the conversion and validation\n",
    "if __name__ == \"__main__\":\n",
    "    result = json_to_xml('books_datav2.json', 'books_datav2.xml')\n",
    "    print(f\"\\nConversion and validation {'successful' if result else 'failed'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5214ccfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Access Denied.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Connect to BaseX server\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mBaseXClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocalhost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1984\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Drop old db if exists\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/BaseXClient/BaseXClient.py:137\u001b[0m, in \u001b[0;36mSession.__init__\u001b[0;34m(self, host, port, user, password, receive_bytes_encoding, send_bytes_encoding)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_response_success():\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccess Denied.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Access Denied.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mquery\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     36\u001b[0m     session\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "from BaseXClient import BaseXClient\n",
    "\n",
    "try:\n",
    "    # Connect to BaseX server\n",
    "    session = BaseXClient.Session('localhost', 1984, 'admin', 'admin')\n",
    "    \n",
    "    # Drop old db if exists\n",
    "    session.execute(\"drop db BookCatalog\")\n",
    "\n",
    "    # Load XML content\n",
    "    with open('books_datav2.xml', 'r', encoding='utf-8') as f:\n",
    "        xml_content = f.read()\n",
    "\n",
    "    # Create new database\n",
    "    session.execute(\"create db BookCatalog \" + xml_content)\n",
    "\n",
    "    # Query database\n",
    "    query = session.query(\"\"\"\n",
    "        for $b in /books/book\n",
    "        where number(substring-after($b/price, '£')) < 11\n",
    "        return data($b/title/text())\n",
    "    \"\"\")\n",
    "\n",
    "    # Print results\n",
    "    for typecode, item in query.iter():\n",
    "        print(\"typecode=%d\" % typecode)\n",
    "        print(\"item=%s\" % str(item))\n",
    "\n",
    "    # Close query and session\n",
    "    query.close()\n",
    "    session.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    query.close()\n",
    "    session.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef89a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
